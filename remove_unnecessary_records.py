import dask.dataframe as dd
import pandas as pd
from tqdm import tqdm


def generate_summary_table(df):
    """
    Generates a new table with the count of total timestamps per day for each ID (maid).

    Parameters:
    df (pd.DataFrame): Input dataframe containing 'maid' and 'datetime' columns.

    Returns:
    pd.DataFrame: A pivot table where rows are 'maid', columns are dates, and values are the count of timestamps.
    """
    # Step 1: Ensure 'maid' and 'datetime' columns are of the right types
    df['maid'] = df['maid'].astype(str)  # Convert 'maid' to string
    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')  # Convert 'datetime' to datetime type

    # Step 2: Create a new 'date' column from 'datetime'
    df['date'] = df['datetime'].dt.date

    # Step 3: Group by 'maid' and 'date' to count the number of timestamps per day for each maid
    grouped = df.groupby(['maid', 'date']).size().reset_index(name='timestamp_count')

    # Step 4: Create a pivot table with 'maid' as index, 'date' as columns, and 'timestamp_count' as values
    pivot_table = grouped.pivot_table(index='maid', columns='date', values='timestamp_count', fill_value=0)

    # Reset index to make 'maid' a column again and for easier viewing
    result = pivot_table.reset_index()

    return result

from tqdm import tqdm

# Enable tqdm progress bar for pandas apply operations
tqdm.pandas()

def filter_summary_table_by_continuous_days(summary_table, min_points, continuous_days):
    """
    Filters the summary table to keep only records where the count of timestamps is greater than or equal to the
    specified minimum points for at least the specified number of continuous days.

    Parameters:
    summary_table (pd.DataFrame): Input dataframe generated from the summary table function, with 'maid' as rows
                                  and dates as columns.
    min_points (int): The minimum number of points (timestamp counts) required per day.
    continuous_days (int): The minimum number of continuous days required with timestamp counts >= min_points.

    Returns:
    pd.DataFrame: Filtered dataframe that meets the criteria.
    """
    # Ensure the index is set to 'maid' for easy manipulation
    summary_table.set_index('maid', inplace=True)
    
    # Define a function to check if a maid has at least `continuous_days` with counts >= min_points
    def check_continuous_days(row):
        # Convert the row to a boolean series: True if value >= min_points, False otherwise
        bool_series = row >= min_points
        
        # Identify continuous segments of days where the condition is met
        count = 0
        max_continuous_days = 0
        
        for value in bool_series:
            if value:
                count += 1  # Increment counter for continuous days
            else:
                max_continuous_days = max(max_continuous_days, count)
                count = 0  # Reset counter if condition is broken
        
        # Ensure we capture the last segment as well
        max_continuous_days = max(max_continuous_days, count)
        
        # Return True if the maximum continuous days is greater than or equal to the specified threshold
        return max_continuous_days >= continuous_days

    # Apply the function to each row with a progress bar using `progress_apply`
    filtered_summary = summary_table[summary_table.progress_apply(check_continuous_days, axis=1)]
    
    # Reset index for better readability
    filtered_summary.reset_index(inplace=True)
    
    return filtered_summary

# Example usage:
# Assuming `summary_table` is the table generated by `generate_summary_table` function.
# summary_table = generate_summary_table(df)
# filtered_summary = filter_summary_table_by_continuous_days(summary_table, min_points=5, continuous_days=3)



def filter_main_dataframe_by_conditions(df, min_points, continuous_days):
    """
    Filters the main dataframe to keep only the records for maids that meet the specified conditions
    of minimum points for a number of continuous days.

    Parameters:
    df (pd.DataFrame): Input main dataframe containing 'maid' and 'datetime' columns.
    min_points (int): Minimum number of points (timestamp counts) required per day.
    continuous_days (int): Minimum number of continuous days required with timestamp counts >= min_points.

    Returns:
    pd.DataFrame: Filtered dataframe containing only records of maids that meet the conditions.
    """
    # Step 1: Generate the summary table with the count of total timestamps per day for each maid
    summary_table = generate_summary_table(df)

    # Step 2: Filter the summary table to find maids that meet the conditions
    filtered_summary = filter_summary_table_by_continuous_days(summary_table, min_points, continuous_days)

    # Step 3: Extract the list of maids that meet the conditions
    maids_meeting_criteria = filtered_summary['maid'].tolist()

    # Step 4: Filter the main dataframe to keep only records of maids that meet the conditions
    filtered_df = df[df['maid'].isin(maids_meeting_criteria)]

    return filtered_df

# Example usage:
# Assuming 'df' is the main dataframe
# filtered_df = filter_main_dataframe_by_conditions(df, min_points=5, continuous_days=3)



def remove_unnecessary_timestamps(df):
    """
    Filters the dataframe to keep only the first and last records where latitude and longitude remain the same
    across consecutive timestamps for each maid, using dask for sorting operations.

    Parameters:
    df (pd.DataFrame): Input pandas dataframe containing 'maid', 'datetime', 'latitude', and 'longitude' columns.

    Returns:
    pd.DataFrame: Filtered pandas dataframe with only the first and last records where coordinates remain the same for each maid.
    """
    # Convert pandas DataFrame to dask DataFrame
    ddf = dd.from_pandas(df, npartitions=4)  # Adjust npartitions as necessary for your dataset size
    # Set the index to 'maid' and specify the number of partitions
    # num_partitions = 10  # You can choose an appropriate number based on your dataset
    # Re-partition to the desired number of partitions
    # ddf = ddf.repartition(npartitions=num_partitions)

    # Round latitude and longitude to 4 decimal places
    ddf['latitude'] = ddf['latitude'].astype(float).round(4)
    ddf['longitude'] = ddf['longitude'].astype(float).round(4)

    # Convert 'datetime' column to datetime type
    ddf['datetime'] = dd.to_datetime(ddf['datetime'], errors='coerce')

    # Sort by 'maid' and 'datetime' to ensure consecutive timestamps are in order within each maid
    # Using dask's sort_values to handle large datasets
    ddf = ddf.sort_values(by=['maid', 'datetime', 'latitude', 'longitude'])

    # Convert back to pandas DataFrame for further operations (since the rest uses pandas syntax)
    df = ddf.compute()

    # Continue with the rest of the operations using pandas
    df = df.reset_index(drop=True)

    # Identify and keep first and last records where lat and long remain the same within each maid
    df['prev_lat'] = df.groupby('maid')['latitude'].shift(1)
    df['prev_long'] = df.groupby('maid')['longitude'].shift(1)
    df['next_lat'] = df.groupby('maid')['latitude'].shift(-1)
    df['next_long'] = df.groupby('maid')['longitude'].shift(-1)

    # Filter to keep rows where latitude or longitude changes within each maid
    filtered_df = df[(df['latitude'] != df['prev_lat']) |
                     (df['longitude'] != df['prev_long']) |
                     (df['latitude'] != df['next_lat']) |
                     (df['longitude'] != df['next_long'])]
    filtered_df.drop(['prev_lat', 'prev_long', 'next_lat', 'next_long'], axis=1, inplace=True)

    # Using tqdm to display a progress bar for group processing
    tqdm.pandas(desc="Processing duplicates based on displacement")

    # Remove records where the same maid has the same timestamp, but check the displacement value
    # Group by 'maid' and 'datetime', and apply custom logic to handle duplicates
    # def process_duplicates(group):
    #     if len(group) == 1:
    #         return group
    #     # If there are duplicates, compare the displacement values
    #     first_row = group.iloc[0]
    #     second_row = group.iloc[1]

    #     # If displacement difference is less than 0.5 km, drop the second row
    #     if abs(first_row['displacement'] - second_row['displacement']) < 0.05:
    #         return group.iloc[[0]]  # Keep the first row only
    #     else:
    #         return group  # Keep both records if displacement >= 0.5 km

    # # Apply the processing to each 'maid' and 'datetime' group with tqdm progress bar
    # filtered_df = filtered_df.groupby(['maid', 'datetime'], group_keys=False).progress_apply(process_duplicates)

    # Drop records for maids with only one record
    # Count occurrences of each maid
    maid_counts = df['maid'].value_counts()
    maid_single_records = maid_counts[maid_counts == 1].index
    filtered_df = filtered_df[~filtered_df['maid'].isin(maid_single_records)]

    return filtered_df

# Example usage:
# Assuming df is your dataframe with the mentioned columns
# filtered_df = remove_unnecessary_timestamps_dask(df)
